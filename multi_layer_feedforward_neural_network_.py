# -*- coding: utf-8 -*-
"""Multi-Layer Feedforward Neural Network .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uv71HN7sERQ0gpOaaLWKgVWD4BVGp8J3

# The First Dataset
"""

from sklearn import datasets
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
n_samples =2000;
np.random.seed(0)
Data, labels = datasets.make_moons(n_samples, noise=0.10)
plt.scatter(Data[:, 0], Data[:, 1], c=labels, s=2, cmap='viridis')

X_train, X_test, y_train, y_test = train_test_split(Data, labels, test_size=0.2, random_state=42)

# Define the sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Neural network architecture
input_size = X_train.shape[1]
hidden_size = 4  # Experiment with different numbers of hidden units
output_size = 2  # Two classes: 0 and 1
learning_rate = 0.1  # Experiment with different learning rates

# Initialize weights and biases
weights_input_hidden = np.random.uniform(size=(input_size, hidden_size))
bias_hidden = np.zeros((1, hidden_size))
weights_hidden_output = np.random.uniform(size=(hidden_size, output_size))
bias_output = np.zeros((1, output_size))




# Training loop


# Convert y_train to a 2D array with one column
y_train = y_train.reshape(-1, 1)

# Training loop
epochs = 1000

for epoch in range(epochs):
    # Forward propagation
    hidden_layer_input = np.dot(X_train, weights_input_hidden) + bias_hidden
    hidden_layer_output = sigmoid(hidden_layer_input)
    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
    output_layer_output = sigmoid(output_layer_input)

    # Calculate loss (binary cross-entropy)
    num_samples = len(y_train)
    loss = -1/num_samples * (y_train * np.log(output_layer_output + 1e-15) + (1 - y_train) * np.log(1 - output_layer_output + 1e-15))

    # Backpropagation
    d_output = (output_layer_output - y_train) / num_samples
    error_hidden = d_output.dot(weights_hidden_output.T)
    d_hidden = error_hidden * hidden_layer_output * (1 - hidden_layer_output)

    # Update weights and biases
    weights_hidden_output -= hidden_layer_output.T.dot(d_output) * learning_rate
    bias_output -= np.sum(d_output, axis=0, keepdims=True) * learning_rate
    weights_input_hidden -= X_train.T.dot(d_hidden) * learning_rate
    bias_hidden -= np.sum(d_hidden, axis=0, keepdims=True) * learning_rate


    if epoch % 50 == 0:
        learning_rate *= 0.9







hidden_layer_input = np.dot(X_test, weights_input_hidden) + bias_hidden
hidden_layer_output = sigmoid(hidden_layer_input)
output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
output_layer_output = sigmoid(output_layer_input)
initial_learning_rate = 0.1
learning_rate_decay = 0.9
for epoch in range(epochs):
    # ... Training code ...
    if epoch % 10 == 0:  # Adjust the schedule frequency as needed
        learning_rate *= learning_rate_decay
# Make predictions and evaluate the model
predictions = np.argmax(output_layer_output, axis=1)
accuracy = np.mean(predictions == y_test)
print(f"Accuracy: {accuracy}")

"""# IRIS dataset"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Load the IRIS dataset from the provided link
iris_data_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
column_names = ["sepal_length", "sepal_width", "petal_length", "petal_width", "class"]
data = pd.read_csv(iris_data_url, names=column_names)

# Preprocess the data
# Map class labels to numerical values (e.g., "Iris-setosa" to 0, "Iris-versicolor" to 1, "Iris-virginica" to 2)
class_mapping = {"Iris-setosa": 0, "Iris-versicolor": 1, "Iris-virginica": 2}
data["class"] = data["class"].map(class_mapping)

# Extract features and labels
X = data.drop("class", axis=1).values
y = data["class"].values

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Neural network architecture
input_size = X_train.shape[1]
hidden_size = 8  # Experiment with different numbers of hidden units
output_size = 3  # For multiclass classification (IRIS has three classes)
initial_learning_rate = 0.1  # Experiment with different learning rates
learning_rate = initial_learning_rate

# Initialize weights and biases
weights_input_hidden = np.random.uniform(size=(input_size, hidden_size))
bias_hidden = np.zeros((1, hidden_size))
weights_hidden_output = np.random.uniform(size=(hidden_size, output_size))
bias_output = np.zeros((1, output_size))

# Training loop
epochs = 1000

for epoch in range(epochs):
    # Forward propagation
    hidden_layer_input = np.dot(X_train, weights_input_hidden) + bias_hidden
    hidden_layer_output = sigmoid(hidden_layer_input)
    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
    output_layer_output = sigmoid(output_layer_input)

    # Calculate loss (multiclass cross-entropy)
    num_samples = len(y_train)
    one_hot_encoded = np.eye(output_size)[y_train]  # Convert labels to one-hot encoding
    loss = -1/num_samples * np.sum(one_hot_encoded * np.log(output_layer_output + 1e-15))

    # Backpropagation
    d_output = (output_layer_output - one_hot_encoded) / num_samples
    error_hidden = d_output.dot(weights_hidden_output.T)
    d_hidden = error_hidden * hidden_layer_output * (1 - hidden_layer_output)

    # Update weights and biases
    weights_hidden_output -= hidden_layer_output.T.dot(d_output) * learning_rate
    bias_output -= np.sum(d_output, axis=0, keepdims=True) * learning_rate
    weights_input_hidden -= X_train.T.dot(d_hidden) * learning_rate
    bias_hidden -= np.sum(d_hidden, axis=0, keepdims=True) * learning_rate

    # Implement learning rate schedule (reducing learning rate every 50 epochs)
    if epoch % 50 == 0:
        learning_rate *= 0.9

# Testing the model
hidden_layer_input = np.dot(X_test, weights_input_hidden) + bias_hidden
hidden_layer_output = sigmoid(hidden_layer_input)
output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
output_layer_output = sigmoid(output_layer_input)

# Make predictions and evaluate the model
predicted_labels = np.argmax(output_layer_output, axis=1)
accuracy = np.mean(predicted_labels == y_test)
print(f"Accuracy: {accuracy}")

